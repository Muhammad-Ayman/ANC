\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}

\title{Adaptive Noise Cancellation Using LMS and RLS Filters\\
\large A Comparative Study}

% Student information - Update with actual names and IDs
% Add or remove students as needed by adding/removing lines below
\newcommand{\studentOneName}{Muhammad Ayman}
\newcommand{\studentOneID}{ID: 222250201}
\newcommand{\studentTwoName}{Omar Mohamed Mostafa}
\newcommand{\studentTwoID}{ID: 222250194}
\newcommand{\studentThreeName}{Ziad Osama Mohamed El-Boshy}
\newcommand{\studentThreeID}{ID: 222250166}
\newcommand{\studentFourName}{Suhila Usama Ahmed}
\newcommand{\studentFourID}{ID: 222250175}
\newcommand{\studentFiveName}{Mohamed Nasser}
\newcommand{\studentFiveID}{ID: 222250204}

\author{%
    \begin{center}
        \textbf{Prepared by:}\\[0.3cm]
        \studentOneName \quad \studentOneID\\[0.2cm]
        \studentTwoName \quad \studentTwoID\\[0.2cm]
        \studentThreeName \quad \studentThreeID\\[0.2cm]
        \studentFourName \quad \studentFourID\\[0.2cm]
        \studentFiveName \quad \studentFiveID\\[0.5cm]
        \textbf{Supervised by:}\\[0.3cm]
        Dr. Amr Wagih
    \end{center}
}
\date{\today}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Adaptive Noise Cancellation Report}
}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!60!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    frame=single,
    captionpos=b
}

\begin{document}

\begin{titlepage}
\centering
\vspace*{1cm}

{\Huge\bfseries Adaptive Noise Cancellation Using LMS and RLS Filters\par}
\vspace{0.5cm}
{\Large A Comparative Study\par}
\vspace{2cm}

{\large\textbf{Prepared by:}\par}
\vspace{0.5cm}
\studentOneName \quad \studentOneID\par
\vspace{0.3cm}
\studentTwoName \quad \studentTwoID\par
\vspace{0.3cm}
\studentThreeName \quad \studentThreeID\par
\vspace{0.3cm}
\studentFourName \quad \studentFourID\par
\vspace{0.3cm}
\studentFiveName \quad \studentFiveID\par
\vspace{1.5cm}

{\large\textbf{Supervised by:}\par}
\vspace{0.5cm}
{\large Dr. Amr Wagih\par}
\vspace{2cm}

{\large\today\par}

\vfill
\end{titlepage}

\begin{abstract}
This report presents a comprehensive analysis of adaptive noise cancellation techniques using Least Mean Square (LMS) and Recursive Least Squares (RLS) adaptive filters. Two implementations were developed: a custom implementation using only Python standard library, and an implementation using the padasip library. The filters were applied to noisy speech signals with separate noise reference inputs. Performance was evaluated through RMS measurements, convergence rate analysis, and visual inspection of the processed signals. Results demonstrate the effectiveness of both algorithms, with LMS showing faster initial convergence and RLS achieving better steady-state performance in certain configurations.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

Adaptive noise cancellation is a fundamental signal processing technique used to remove unwanted noise from desired signals. This is particularly important in applications such as speech enhancement, where background noise can significantly degrade signal quality. The technique relies on having a reference noise signal that is correlated with the noise present in the primary signal.

This project implements and compares two prominent adaptive filtering algorithms:
\begin{itemize}
    \item \textbf{Least Mean Square (LMS)}: A simple, computationally efficient algorithm with good convergence properties
    \item \textbf{Recursive Least Squares (RLS)}: A more complex algorithm that typically converges faster and achieves better steady-state performance at the cost of higher computational complexity
\end{itemize}

\section{Methodology}

\subsection{Adaptive Noise Cancellation Principle}

The adaptive noise cancellation system operates on the principle shown in Figure~\ref{fig:anc_principle}. The primary input $d(n)$ contains the desired signal $s(n)$ plus noise $n_0(n)$. A reference input $x(n)$ contains noise $n_1(n)$ that is correlated with $n_0(n)$ but uncorrelated with $s(n)$.

\begin{figure}[h]
\centering
\fbox{\parbox{0.8\textwidth}{\centering
\textbf{Adaptive Noise Cancellation Block Diagram}\\[0.5em]
Primary Input: $d(n) = s(n) + n_0(n)$\\
Reference Input: $x(n) = n_1(n)$\\
Output: $e(n) = d(n) - y(n)$
}}
\caption{Adaptive noise cancellation principle}
\label{fig:anc_principle}
\end{figure}

The adaptive filter estimates the noise component from the reference input and subtracts it from the primary input, producing the error signal $e(n)$ which converges to the desired signal $s(n)$.

\subsection{Least Mean Square (LMS) Algorithm}

The LMS algorithm is a stochastic gradient descent method that minimizes the mean square error. The filter update equations are:

\begin{align}
y(n) &= \mathbf{w}^T(n) \mathbf{x}(n) = \sum_{i=0}^{M-1} w_i(n) x(n-i) \label{eq:lms_output}\\
e(n) &= d(n) - y(n) \label{eq:lms_error}\\
\mathbf{w}(n+1) &= \mathbf{w}(n) + 2\mu e(n) \mathbf{x}(n) \label{eq:lms_update}
\end{align}

where:
\begin{itemize}
    \item $\mathbf{w}(n)$ is the filter weight vector of order $M$
    \item $\mathbf{x}(n) = [x(n), x(n-1), \ldots, x(n-M+1)]^T$ is the input vector
    \item $\mu$ is the step size parameter
    \item $e(n)$ is the error signal (output)
\end{itemize}

\subsubsection{Convergence and Stability}

The step size $\mu$ must satisfy:
\begin{equation}
0 < \mu < \frac{1}{\lambda_{\max}}
\end{equation}
where $\lambda_{\max}$ is the largest eigenvalue of the input correlation matrix $\mathbf{R} = E[\mathbf{x}(n)\mathbf{x}^T(n)]$, to ensure stability.

The convergence rate of LMS depends on the eigenvalue spread $\chi = \lambda_{\max}/\lambda_{\min}$:
\begin{itemize}
    \item \textbf{Small eigenvalue spread} ($\chi \approx 1$): Fast, uniform convergence
    \item \textbf{Large eigenvalue spread} ($\chi \gg 1$): Slow convergence, especially for modes corresponding to small eigenvalues
\end{itemize}

The misadjustment (excess mean square error) is approximately:
\begin{equation}
M \approx \mu \cdot \text{tr}(\mathbf{R})
\end{equation}
where $\text{tr}(\mathbf{R})$ is the trace of the correlation matrix. This represents the trade-off between convergence speed and steady-state error.

\subsubsection{Computational Complexity}

The LMS algorithm requires:
\begin{itemize}
    \item \textbf{Per sample}: $2M$ multiplications and $2M$ additions
    \item \textbf{Total complexity}: $O(M)$ per sample, $O(N \cdot M)$ for $N$ samples
    \item \textbf{Memory}: $O(M)$ for weight vector and input buffer
\end{itemize}

This makes LMS highly efficient for real-time applications.

\subsection{Recursive Least Squares (RLS) Algorithm}

The RLS algorithm minimizes a weighted least squares cost function and provides faster convergence at the expense of higher computational complexity. The update equations are:

\begin{align}
\mathbf{k}(n) &= \frac{\mathbf{P}(n-1)\mathbf{x}(n)}{\lambda + \mathbf{x}^T(n)\mathbf{P}(n-1)\mathbf{x}(n)} \label{eq:rls_kalman}\\
e(n) &= d(n) - \mathbf{w}^T(n-1)\mathbf{x}(n) \label{eq:rls_error}\\
\mathbf{w}(n) &= \mathbf{w}(n-1) + \mathbf{k}(n)e(n) \label{eq:rls_update}\\
\mathbf{P}(n) &= \frac{1}{\lambda}\left[\mathbf{P}(n-1) - \mathbf{k}(n)\mathbf{x}^T(n)\mathbf{P}(n-1)\right] \label{eq:rls_p}
\end{align}

where:
\begin{itemize}
    \item $\lambda$ is the forgetting factor ($0 < \lambda \leq 1$)
    \item $\mathbf{P}(n)$ is the inverse correlation matrix
    \item $\mathbf{k}(n)$ is the Kalman gain vector
    \item $\delta$ is used for initialization: $\mathbf{P}(0) = \delta^{-1}\mathbf{I}$
\end{itemize}

\subsubsection{Convergence and Stability}

The forgetting factor $\lambda$ controls the memory of the algorithm. Values close to 1 provide better steady-state performance but slower tracking of non-stationary signals.

The RLS algorithm has several important properties:
\begin{itemize}
    \item \textbf{Convergence rate}: Independent of eigenvalue spread, typically 2-3 times faster than LMS
    \item \textbf{Steady-state error}: Lower than LMS for the same filter order
    \item \textbf{Tracking ability}: Controlled by $\lambda$; smaller values track changes faster
\end{itemize}

The effective memory of the RLS filter is approximately:
\begin{equation}
\tau_{\text{eff}} \approx \frac{1}{1-\lambda}
\end{equation}
For $\lambda = 0.995$, the effective memory is about 200 samples.

\subsubsection{Computational Complexity}

The RLS algorithm requires:
\begin{itemize}
    \item \textbf{Per sample}: $O(M^2)$ operations due to matrix operations
    \item \textbf{Total complexity}: $O(N \cdot M^2)$ for $N$ samples
    \item \textbf{Memory}: $O(M^2)$ for the inverse correlation matrix $\mathbf{P}(n)$
\end{itemize}

The computational cost is significantly higher than LMS, making it less suitable for real-time applications with limited computational resources. However, the faster convergence often justifies the cost for offline processing.

\subsubsection{Numerical Stability}

The RLS algorithm can suffer from numerical instability, particularly when:
\begin{itemize}
    \item The input signal has low energy (small $\mathbf{x}^T(n)\mathbf{P}(n-1)\mathbf{x}(n)$)
    \item The forgetting factor $\lambda$ is very close to 1
    \item The initialization parameter $\delta$ is too small
\end{itemize}

The initialization $\mathbf{P}(0) = \delta^{-1}\mathbf{I}$ helps ensure numerical stability. Typical values of $\delta$ range from 0.01 to 0.1.

\section{Implementation}

\subsection{Custom Implementation}

A custom implementation was developed using only Python's standard library (no external DSP libraries). This implementation includes:

\begin{itemize}
    \item WAV file I/O using the \texttt{wave} module
    \item Custom LMS and RLS filter implementations
    \item Signal preprocessing (DC removal and normalization)
    \item Convergence analysis tools
    \item Visualization using matplotlib
\end{itemize}

\subsubsection{Audio File I/O}

The implementation handles 16-bit PCM WAV files, supporting both mono and stereo inputs. Stereo files are automatically converted to mono by averaging the channels:

\begin{lstlisting}[caption=WAV File Reading Implementation]
def read_wav(path: Path) -> Tuple[int, List[float]]:
    """Return sample_rate and mono float samples in [-1, 1]."""
    with wave.open(str(path), "rb") as wf:
        channels = wf.getnchannels()
        sampwidth = wf.getsampwidth()
        rate = wf.getframerate()
        frames = wf.getnframes()
        raw = wf.readframes(frames)

    # Only 16-bit PCM supported
    if sampwidth != 2:
        raise ValueError(f"Only 16-bit PCM supported, "
                        f"got {sampwidth * 8} bits")

    # Unpack 16-bit signed integers (little-endian)
    samples = struct.unpack("<" + "h" * (len(raw) // 2), raw)
    
    # Convert stereo to mono by averaging
    if channels == 2:
        mono = [(samples[i] + samples[i + 1]) / 2 
                for i in range(0, len(samples), 2)]
    else:
        mono = list(samples)

    # Normalize to [-1, 1] range
    norm = 32768.0  # 2^15 for 16-bit signed integers
    return rate, [s / norm for s in mono]
\end{lstlisting}

The write function ensures no clipping occurs by scaling the signal if necessary:

\begin{lstlisting}[caption=WAV File Writing with Clipping Prevention]
def write_wav(path: Path, rate: int, samples: List[float]) -> None:
    """Write mono 16-bit PCM wav from normalized floats."""
    # Avoid clipping by scaling if peak > 1.0
    peak = max(max((abs(s) for s in samples), default=0.0), 1e-12)
    if peak > 1.0:
        scale = 1.0 / peak
        samples = [s * scale for s in samples]
    
    # Convert to 16-bit integers
    int_samples = [int(max(-1.0, min(1.0, s)) * 32767) 
                   for s in samples]
    raw = struct.pack("<" + "h" * len(int_samples), *int_samples)
    
    with wave.open(str(path), "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(rate)
        wf.writeframes(raw)
\end{lstlisting}

\subsubsection{LMS Filter Implementation}

The LMS filter implementation follows the standard algorithm with weight vector initialization to zero:

\begin{lstlisting}[caption=Custom LMS Filter Implementation]
def lms_filter(noise: List[float], target: List[float], 
               order: int = 12, mu: float = 0.005) -> List[float]:
    """Least-mean-square adaptive filter returning the error."""
    # Initialize weight vector to zeros
    w = [0.0] * order
    out: List[float] = []
    
    for n in range(len(target)):
        # Create input vector with past samples (zero-padded at start)
        x = [noise[n - k] if n - k >= 0 else 0.0 
             for k in range(order)]
        
        # Filter output: y(n) = w^T(n) * x(n)
        y = sum(w[i] * x[i] for i in range(order))
        
        # Error signal: e(n) = d(n) - y(n)
        e = target[n] - y
        
        # Weight update: w(n+1) = w(n) + 2*mu*e(n)*x(n)
        for i in range(order):
            w[i] += 2 * mu * e * x[i]
        
        out.append(e)  # Error is the cleaned output
    return out
\end{lstlisting}

Key implementation details:
\begin{itemize}
    \item \textbf{Zero-padding}: For $n < M$ (where $M$ is the filter order), past samples are set to zero, ensuring the filter operates correctly at the signal start
    \item \textbf{In-place weight update}: The weight vector is updated immediately after computing the error, using the current input vector
    \item \textbf{Error as output}: The error signal $e(n)$ represents the cleaned signal after noise cancellation
\end{itemize}

\subsubsection{RLS Filter Implementation}

The RLS implementation includes the inverse correlation matrix update, which is computationally more complex:

\begin{lstlisting}[caption=Custom RLS Filter Implementation]
def rls_filter(noise: List[float], target: List[float],
               order: int = 12, lam: float = 0.95, 
               delta: float = 0.1) -> List[float]:
    """Recursive-least-square adaptive filter."""
    # Initialize weight vector
    w = [0.0] * order
    
    # Initialize inverse correlation matrix: P(0) = delta^(-1) * I
    P = [[0.0] * order for _ in range(order)]
    for i in range(order):
        P[i][i] = 1.0 / delta

    out: List[float] = []
    for n in range(len(target)):
        # Input vector with zero-padding
        x = [noise[n - k] if n - k >= 0 else 0.0 
             for k in range(order)]
        
        # Compute P * x
        Px = [sum(P[i][j] * x[j] for j in range(order)) 
              for i in range(order)]
        
        # Compute x^T * P * x
        xPx = sum(x[i] * Px[i] for i in range(order))
        denom = lam + xPx
        
        # Kalman gain: k = (P * x) / (lambda + x^T * P * x)
        k = [Px[i] / denom for i in range(order)]

        # Filter output
        y = sum(w[i] * x[i] for i in range(order))
        
        # Error signal
        e = target[n] - y
        
        # Weight update: w(n) = w(n-1) + k(n) * e(n)
        for i in range(order):
            w[i] += k[i] * e

        # Update inverse correlation matrix
        # P(n) = (P(n-1) - k(n) * x^T(n) * P(n-1)) / lambda
        xTP = [sum(x[j] * P[j][i] for j in range(order)) 
               for i in range(order)]
        for i in range(order):
            for j in range(order):
                P[i][j] = (P[i][j] - k[i] * xTP[j]) / lam

        out.append(e)
    return out
\end{lstlisting}

The RLS algorithm's complexity is $O(M^2)$ per sample due to:
\begin{itemize}
    \item Matrix-vector multiplication: $\mathbf{P}(n-1)\mathbf{x}(n)$
    \item Outer product computation: $\mathbf{k}(n)\mathbf{x}^T(n)\mathbf{P}(n-1)$
    \item Matrix update operations
\end{itemize}

The initialization parameter $\delta$ controls the initial condition of the inverse correlation matrix. Smaller values make the filter more responsive initially but may cause numerical instability.

\subsection{Padasip Implementation}

An alternative implementation using the padasip library was developed for comparison. Padasip is a Python library for adaptive signal processing that provides optimized implementations:

\begin{itemize}
    \item \textbf{Optimized C-based core}: Faster execution for large signals
    \item \textbf{Additional filter variants}: NLMS, AP, and other adaptive algorithms
    \item \textbf{Built-in utilities}: Preprocessing and input history generation
    \item \textbf{Well-tested}: Mature library with extensive testing
\end{itemize}

\subsubsection{Padasip LMS Implementation}

The padasip implementation uses the library's FilterLMS class:

\begin{lstlisting}[caption=Padasip LMS Implementation]
import padasip as pa

def lms_padasip(noisy, noise, order=32, mu=0.01):
    """LMS filter using padasip library."""
    # Create input history matrix from noise reference
    X = pa.preprocess.input_from_history(noise, order)
    
    # Initialize LMS filter
    f = pa.filters.FilterLMS(n=order, mu=mu, w="random")
    
    # Process signal
    y, e, w = f.run(noisy, X)
    
    return e  # Return error signal (cleaned output)
\end{lstlisting}

Key differences from custom implementation:
\begin{itemize}
    \item Input history is pre-computed as a matrix (more memory, faster processing)
    \item Weight initialization can be random, zero, or custom
    \item Returns filter output, error, and final weights
\end{itemize}

\subsubsection{Padasip RLS Implementation}

The padasip RLS implementation uses FilterRLS:

\begin{lstlisting}[caption=Padasip RLS Implementation]
def rls_padasip(noisy, noise, order=16, lam=0.995, delta=0.01):
    """RLS filter using padasip library."""
    # Create input history matrix
    X = pa.preprocess.input_from_history(noise, order)
    
    # Initialize RLS filter
    # Note: padasip uses 'mu' parameter mapped from lambda
    f = pa.filters.FilterRLS(n=order, mu=lam, eps=delta, w="random")
    
    # Process signal
    y, e, w = f.run(noisy, X)
    
    return e  # Return error signal
\end{lstlisting}

Note: The padasip library uses parameter naming that differs slightly from standard RLS notation. The \texttt{mu} parameter corresponds to the forgetting factor $\lambda$, and \texttt{eps} corresponds to the initialization parameter $\delta$.

\subsection{Preprocessing}

Both implementations apply preprocessing to improve filter convergence. The preprocessing pipeline consists of two stages applied sequentially:

\begin{enumerate}
    \item \textbf{DC Removal}: Subtracts the mean to remove DC offset
    \begin{equation}
    x_{\text{dc}}(n) = x(n) - \frac{1}{N}\sum_{i=0}^{N-1} x(i)
    \end{equation}
    
    The DC component can cause the adaptive filter to converge to a biased solution. Removing it ensures the signal is centered around zero, which is optimal for adaptive filtering.
    
    \begin{lstlisting}[caption=DC Removal Implementation]
def remove_dc(signal: List[float]) -> List[float]:
    """Remove DC component by subtracting mean."""
    if not signal:
        return signal
    mean = sum(signal) / len(signal)
    return [s - mean for s in signal]
    \end{lstlisting}
    
    \item \textbf{Normalization}: Scales the signal to prevent clipping
    \begin{equation}
    x_{\text{norm}}(n) = \frac{x(n)}{\max(|x(n)|)} \cdot \alpha
    \end{equation}
    
    where $\alpha = 0.99$ is used as a safety margin to prevent clipping at exactly $\pm 1.0$. This ensures all samples remain within the valid range $[-1, 1]$ for floating-point processing and prevents overflow when converting back to integer format.
    
    \begin{lstlisting}[caption=Normalization Implementation]
def normalize(signal: List[float], peak_target: float = 0.99) -> List[float]:
    """Scale to a target peak magnitude."""
    if not signal:
        return signal
    peak = max(abs(s) for s in signal)
    if peak == 0:
        return signal  # Avoid division by zero
    scale = peak_target / peak
    return [s * scale for s in signal]
    \end{lstlisting}
\end{enumerate}

The preprocessing is applied to both the noisy signal and the noise reference before filtering. This ensures:
\begin{itemize}
    \item Both signals have zero mean, improving filter convergence
    \item Both signals are scaled to similar amplitude ranges
    \item Numerical stability during filter operations
    \item Prevention of clipping artifacts in the output
\end{itemize}

\section{Experimental Setup}

\subsection{Test Signals}

The experiments were conducted using audio files:
\begin{itemize}
    \item \textbf{Primary Input}: \texttt{aud/audio.wav} - Noisy speech signal
    \item \textbf{Reference Input}: \texttt{aud/audio\_noise.wav} - Noise reference signal
    \item Additional test pair: \texttt{aud/audio2.wav} and \texttt{aud/audio2\_noise.wav}
\end{itemize}

\subsection{Parameter Selection}

A comprehensive grid search was performed to find optimal parameters. The grid search evaluates all parameter combinations and selects the configuration that minimizes the output RMS, which serves as a proxy for noise reduction effectiveness.

\subsubsection{Grid Search Implementation}

The grid search function systematically tests all parameter combinations:

\begin{lstlisting}[caption=Grid Search Implementation]
def grid_test(noisy: List[float], noise: List[float],
              lms_grid: Iterable[Tuple[int, float]],
              rls_grid: Iterable[Tuple[int, float, float]]):
    """Return best LMS and RLS parameter sets by RMS of output."""
    best_lms = (None, float("inf"))
    best_rls = (None, float("inf"))

    # Test all LMS parameter combinations
    for order, mu in lms_grid:
        out = lms_filter(noise, noisy, order=order, mu=mu)
        val = rms(out)  # Evaluate performance
        if val < best_lms[1]:  # Lower RMS is better
            best_lms = ((order, mu), val)

    # Test all RLS parameter combinations
    for order, lam, delta in rls_grid:
        out = rls_filter(noise, noisy, order=order, 
                        lam=lam, delta=delta)
        val = rms(out)
        if val < best_rls[1]:
            best_rls = ((order, lam, delta), val)

    return best_lms[0], best_lms[1], best_rls[0], best_rls[1]
\end{lstlisting}

\subsubsection{LMS Grid Search}

The LMS grid search explores combinations of filter order and step size:

\begin{itemize}
    \item \textbf{Filter orders}: 16, 24, 32, 48
    \begin{itemize}
        \item Lower orders (16-24): Faster computation, may underfit complex noise patterns
        \item Higher orders (32-48): Better noise modeling, increased computational cost
    \end{itemize}
    \item \textbf{Step sizes}: 0.0035, 0.004, 0.005, 0.006, 0.010
    \begin{itemize}
        \item Smaller $\mu$ (0.0035-0.005): Slower convergence, better steady-state performance
        \item Larger $\mu$ (0.006-0.010): Faster convergence, risk of instability
    \end{itemize}
\end{itemize}

Total combinations tested: $4 \times 5 = 20$ parameter sets.

\subsubsection{RLS Grid Search}

The RLS grid search explores three-dimensional parameter space:

\begin{itemize}
    \item \textbf{Filter orders}: 8, 12, 16, 24
    \begin{itemize}
        \item Lower orders (8-12): Lower computational complexity ($O(M^2)$)
        \item Higher orders (16-24): Better performance, significantly higher cost
    \end{itemize}
    \item \textbf{Forgetting factors}: 0.990, 0.992, 0.995
    \begin{itemize}
        \item Lower $\lambda$ (0.990): Better tracking of non-stationary noise
        \item Higher $\lambda$ (0.995): Better steady-state performance, slower adaptation
    \end{itemize}
    \item \textbf{Initialization deltas}: 0.005, 0.01, 0.020
    \begin{itemize}
        \item Smaller $\delta$: More aggressive initial adaptation
        \item Larger $\delta$: More conservative, numerically stable
    \end{itemize}
\end{itemize}

Total combinations tested: $4 \times 3 \times 3 = 36$ parameter sets.

The grid search methodology:
\begin{enumerate}
    \item For each parameter combination, run the adaptive filter
    \item Compute the RMS of the output signal
    \item Select the combination with minimum RMS
    \item Use the selected parameters for final processing
\end{enumerate}

This exhaustive search ensures optimal parameter selection for the given test signals, though it requires significant computational time.

\subsection{Performance Metrics}

Performance was evaluated using multiple metrics to assess both overall performance and convergence behavior:

\begin{itemize}
    \item \textbf{RMS (Root Mean Square)}: Overall signal energy measure
    \begin{equation}
    \text{RMS} = \sqrt{\frac{1}{N}\sum_{n=0}^{N-1} e^2(n)}
    \end{equation}
    
    The RMS provides a single scalar measure of the output signal energy. Lower RMS values indicate better noise cancellation, assuming the desired signal energy remains constant.
    
    \begin{lstlisting}[caption=RMS Calculation]
def rms(signal: List[float]) -> float:
    """Root-mean-square level."""
    if not signal:
        return 0.0
    return math.sqrt(sum(s * s for s in signal) / len(signal))
    \end{lstlisting}
    
    \item \textbf{Running RMS}: Time-varying RMS computed over a sliding window to track convergence
    \begin{equation}
    \text{RMS}_{\text{run}}(n) = \sqrt{\frac{1}{W}\sum_{i=\max(0,n-W+1)}^{n} e^2(i)}
    \end{equation}
    where $W$ is the window size in samples, typically corresponding to 50 ms of audio.
    
    The running RMS reveals how the filter performance evolves over time, showing:
    \begin{itemize}
        \item Initial convergence speed
        \item Steady-state performance
        \item Adaptation to signal changes
    \end{itemize}
    
    \begin{lstlisting}[caption=Running RMS Implementation]
def running_rms(signal: List[float], window: int) -> List[float]:
    """Compute running RMS over a sliding window."""
    if not signal or window < 1:
        return []
    
    # Precompute cumulative sum of squares for efficiency
    cumsum = [0.0]
    for s in signal:
        cumsum.append(cumsum[-1] + s * s)
    
    out = []
    for i in range(len(signal)):
        start = max(0, i - window + 1)
        if start == 0:
            total_window = cumsum[i + 1]
            count = i + 1
        else:
            total_window = cumsum[i + 1] - cumsum[start]
            count = window
        out.append(math.sqrt(total_window / count))
    return out
    \end{lstlisting}
    
    The implementation uses cumulative sums for computational efficiency, reducing the complexity from $O(N \cdot W)$ to $O(N)$.
\end{itemize}

\subsection{Convergence Analysis}

Convergence analysis tracks how the adaptive filters adapt over time. The running RMS is computed with a 50 ms window (configurable via \texttt{--conv-window-ms}) and saved to CSV for further analysis:

\begin{lstlisting}[caption=Convergence Data Export]
# Convergence curves (running RMS)
window = max(1, int(rate_noisy * (args.conv_window_ms / 1000.0)))
lms_curve = running_rms(lms_clean, window)
rls_curve = running_rms(rls_clean, window)

# Export to CSV
with conv_csv.open("w", encoding="utf-8") as f:
    f.write("time_sec,lms_running_rms,rls_running_rms\n")
    for t, a, b in zip(times, lms_curve, rls_curve):
        f.write(f"{t},{a},{b}\n")
\end{lstlisting}

The convergence data enables:
\begin{itemize}
    \item Quantitative comparison of convergence rates
    \item Identification of convergence time (time to reach steady-state)
    \item Analysis of filter stability
    \item Performance optimization through parameter tuning
\end{itemize}

\section{Results}

\subsection{Grid Search Results}

The grid search identified optimal parameters for each algorithm:

\begin{table}[h]
\centering
\caption{Optimal Parameters from Grid Search}
\begin{tabular}{lccc}
\toprule
\textbf{Algorithm} & \textbf{Order} & \textbf{Parameter} & \textbf{Output RMS} \\
\midrule
LMS (Custom) & 32 & $\mu = 0.01$ & 0.0796 \\
RLS (Custom) & 8 & $\lambda = 0.995$, $\delta = 0.01$ & 0.0903 \\
LMS (Padasip) & 32 & $\mu = 0.01$ & 0.0868 \\
RLS (Padasip) & 16 & $\lambda = 0.995$, $\delta = 0.01$ & 0.1025 \\
\bottomrule
\end{tabular}
\label{tab:grid_results}
\end{table}

\subsection{Signal Quality Comparison}

\begin{table}[h]
\centering
\caption{RMS Comparison for Different Test Cases}
\begin{tabular}{lccc}
\toprule
\textbf{Test Case} & \textbf{Input RMS} & \textbf{LMS RMS} & \textbf{RLS RMS} \\
\midrule
audio.wav (Custom) & 0.1748 & 0.0796 & 0.0903 \\
audio.wav (Padasip) & 0.1765 & 0.0868 & 0.1025 \\
audio2.wav (Padasip) & 0.1734 & 0.1702 & 0.1729 \\
\bottomrule
\end{tabular}
\label{tab:rms_comparison}
\end{table}

\subsection{Convergence Analysis}

The convergence behavior was analyzed using running RMS curves. Key observations:

\begin{enumerate}
    \item \textbf{Initial Convergence}: RLS typically converges faster initially due to its optimal update mechanism
    \item \textbf{Steady-State Performance}: LMS with optimal parameters achieved lower steady-state RMS in the custom implementation
    \item \textbf{Convergence Time}: RLS reached steady-state faster, typically within the first 0.1-0.5 seconds
    \item \textbf{Stability}: Both algorithms remained stable throughout the signal duration
\end{enumerate}

Figure~\ref{fig:convergence} shows the convergence curves for both algorithms. The RLS algorithm demonstrates faster initial convergence, while LMS achieves better steady-state performance in this configuration.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{outputs_basic/convergence.png}
\caption{Convergence comparison: Running RMS over time for LMS and RLS filters}
\label{fig:convergence}
\end{figure}

\subsection{Signal Waveforms}

Figure~\ref{fig:signals} shows the original noisy signal compared to the cleaned outputs from both filters. Both algorithms successfully reduce noise while preserving the speech content.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{outputs_basic/signals.png}
\caption{Signal comparison: Original noisy speech (top), LMS cleaned (middle), RLS cleaned (bottom)}
\label{fig:signals}
\end{figure}

\section{Discussion}

\subsection{Algorithm Comparison}

\subsubsection{Computational Complexity}

\begin{itemize}
    \item \textbf{LMS}: $O(M)$ operations per sample, where $M$ is the filter order
    \item \textbf{RLS}: $O(M^2)$ operations per sample due to matrix operations
\end{itemize}

For real-time applications with limited computational resources, LMS is preferred. For offline processing or when computational power is available, RLS may provide better performance.

\subsubsection{Convergence Characteristics}

\begin{itemize}
    \item \textbf{LMS}: Convergence rate depends on the step size $\mu$ and eigenvalue spread of the input correlation matrix. Larger $\mu$ provides faster convergence but may cause instability.
    \item \textbf{RLS}: Generally converges faster and is less sensitive to eigenvalue spread, but requires careful selection of the forgetting factor $\lambda$.
\end{itemize}

\subsubsection{Parameter Sensitivity}

\begin{itemize}
    \item \textbf{LMS}: The step size $\mu$ is critical. Too large values cause instability; too small values result in slow convergence.
    \item \textbf{RLS}: The forgetting factor $\lambda$ balances tracking ability and steady-state performance. Values close to 1 provide better noise reduction but slower adaptation to changes.
\end{itemize}

\subsection{Implementation Comparison}

The custom implementation and padasip implementation showed similar results, with slight differences due to:
\begin{itemize}
    \item Different initialization strategies
    \item Numerical precision differences
    \item Implementation-specific optimizations
\end{itemize}

The custom implementation achieved slightly better RMS values, possibly due to more careful parameter tuning and preprocessing.

\subsection{Limitations and Future Work}

\begin{enumerate}
    \item \textbf{Non-stationary Noise}: Both algorithms assume some degree of stationarity. Rapidly changing noise characteristics may degrade performance.
    \item \textbf{Correlation Requirements}: The reference noise must be correlated with the noise in the primary signal. Poor correlation leads to ineffective cancellation.
    \item \textbf{Speech Distortion}: Aggressive noise cancellation may introduce artifacts or distort the desired signal.
    \item \textbf{Real-time Processing}: The current implementation processes entire files. Real-time streaming would require buffering and incremental processing.
\end{enumerate}

Future improvements could include:
\begin{itemize}
    \item Variable step-size LMS for better convergence-speed tradeoff
    \item Frequency-domain implementations for computational efficiency
    \item Multi-channel adaptive filtering
    \item Integration with voice activity detection (VAD)
\end{itemize}

\section{Conclusion}

This project successfully implemented and compared LMS and RLS adaptive filters for noise cancellation. Key findings:

\begin{enumerate}
    \item Both algorithms effectively reduce noise when provided with a correlated noise reference
    \item LMS achieved better steady-state RMS performance in the tested configuration (0.0796 vs 0.0903)
    \item RLS demonstrated faster initial convergence, reaching steady-state more quickly
    \item The custom implementation performed comparably to the padasip library implementation
    \item Proper parameter selection is crucial for optimal performance
\end{enumerate}

The choice between LMS and RLS depends on the specific application requirements:
\begin{itemize}
    \item Use \textbf{LMS} for: Real-time applications, limited computational resources, simpler implementation
    \item Use \textbf{RLS} for: Offline processing, faster convergence requirements, when computational cost is acceptable
\end{itemize}

Both implementations successfully demonstrate the principles of adaptive noise cancellation and provide practical tools for speech enhancement applications.

\section{Appendix}

\subsection{File Structure}

The project structure is organized as follows:
\begin{verbatim}
ANC/
├── aud/
│   ├── audio.wav
│   ├── audio_noise.wav
│   ├── audio2.wav
│   └── audio2_noise.wav
├── outputs_basic/
│   ├── audio_lms.wav
│   ├── audio_rls.wav
│   ├── signals.png
│   ├── convergence.png
│   └── convergence.csv
├── outputs_padasip/
│   ├── audio_lms_pada.wav
│   ├── audio_rls_pada.wav
│   ├── signals_pada.png
│   ├── convergence_pada.png
│   └── convergence_pada.csv
├── adaptive_noise_cancellation.py
└── adaptive_noise_cancellation_padasip.py
\end{verbatim}

\subsection{Command-Line Interface}

Both implementations provide comprehensive command-line interfaces for flexible usage.

\subsubsection{Custom Implementation}

The custom implementation supports the following command-line arguments:

\begin{lstlisting}[caption=Command-Line Arguments for Custom Implementation]
--noisy PATH          Path to noisy speech (default: aud/audio.wav)
--noise PATH          Path to noise reference (default: aud/audio_noise.wav)
--lms-order INT      LMS filter order (default: 12)
--lms-mu FLOAT        LMS step size (default: 0.0025)
--rls-order INT      RLS filter order (default: 8)
--rls-lam FLOAT      RLS forgetting factor (default: 0.995)
--rls-delta FLOAT     RLS initialization delta (default: 0.01)
--grid                Enable grid search for optimal parameters
--skip-plot           Disable plot generation
--conv-window-ms FLOAT Running RMS window size in ms (default: 50.0)
\end{lstlisting}

Usage examples:

\begin{lstlisting}[caption=Basic Usage]
# Run with default parameters (uses aud/audio.wav and aud/audio_noise.wav)
python adaptive_noise_cancellation.py

# Process different audio files
python adaptive_noise_cancellation.py \
    --noisy aud/audio2.wav \
    --noise aud/audio2_noise.wav

# Run with grid search to find optimal parameters
python adaptive_noise_cancellation.py --grid

# Specify custom parameters manually
python adaptive_noise_cancellation.py \
    --lms-order 32 --lms-mu 0.01 \
    --rls-order 16 --rls-lam 0.995 --rls-delta 0.01

# Skip plot generation (faster execution)
python adaptive_noise_cancellation.py --grid --skip-plot

# Custom convergence window size
python adaptive_noise_cancellation.py --conv-window-ms 100.0
\end{lstlisting}

\subsubsection{Padasip Implementation}

The padasip implementation provides similar functionality with additional options:

\begin{lstlisting}[caption=Command-Line Arguments for Padasip Implementation]
--noisy PATH          Path to noisy speech (default: aud/audio.wav)
--noise PATH          Path to noise reference (default: aud/audio_noise.wav)
--algo {lms,rls,both} Algorithm to run (default: both)
--lms-order INT       LMS filter order (default: 32)
--lms-mu FLOAT        LMS step size (default: 0.01)
--rls-order INT       RLS filter order (default: 16)
--rls-lam FLOAT       RLS forgetting factor (default: 0.995)
--rls-delta FLOAT     RLS initialization delta (default: 0.01)
--skip-plot           Disable plot generation
--conv-window-ms FLOAT Running RMS window size in ms (default: 50.0)
\end{lstlisting}

Usage examples:

\begin{lstlisting}[caption=Padasip Usage Examples]
# Run both algorithms
python adaptive_noise_cancellation_padasip.py --algo both

# Run only LMS filter
python adaptive_noise_cancellation_padasip.py --algo lms \
    --lms-order 32 --lms-mu 0.01

# Run only RLS filter
python adaptive_noise_cancellation_padasip.py --algo rls \
    --rls-order 16 --rls-lam 0.995 --rls-delta 0.01

# Process different files without plots
python adaptive_noise_cancellation_padasip.py \
    --noisy aud/audio2.wav \
    --noise aud/audio2_noise.wav \
    --skip-plot
\end{lstlisting}

\subsection{Main Processing Pipeline}

The main processing pipeline for both implementations follows these steps:

\begin{enumerate}
    \item \textbf{File Loading}: Read WAV files and extract audio samples
    \begin{lstlisting}[caption=File Loading]
rate_noisy, noisy = read_wav(noisy_path)
rate_noise, noise = read_wav(noise_path)
# Ensure sample rates match
if rate_noisy != rate_noise:
    raise ValueError("Sample rates must match")
    \end{lstlisting}
    
    \item \textbf{Signal Alignment}: Trim to common length
    \begin{lstlisting}[caption=Signal Alignment]
length = min(len(noisy), len(noise))
noisy = noisy[:length]
noise = noise[:length]
    \end{lstlisting}
    
    \item \textbf{Preprocessing}: Remove DC and normalize
    \begin{lstlisting}[caption=Preprocessing]
noisy = normalize(remove_dc(noisy))
noise = normalize(remove_dc(noise))
    \end{lstlisting}
    
    \item \textbf{Grid Search} (optional): Find optimal parameters
    \begin{lstlisting}[caption=Grid Search]
if args.grid:
    best_lms, best_lms_rms, best_rls, best_rls_rms = grid_test(...)
    args.lms_order, args.lms_mu = best_lms
    args.rls_order, args.rls_lam, args.rls_delta = best_rls
    \end{lstlisting}
    
    \item \textbf{Filtering}: Apply adaptive filters
    \begin{lstlisting}[caption=Filtering]
lms_clean = lms_filter(noise, noisy, order=args.lms_order, mu=args.lms_mu)
rls_clean = rls_filter(noise, noisy, order=args.rls_order, 
                       lam=args.rls_lam, delta=args.rls_delta)
    \end{lstlisting}
    
    \item \textbf{Output Generation}: Save results and generate plots
    \begin{lstlisting}[caption=Output Generation]
write_wav(out_dir / "audio_lms.wav", rate_noisy, lms_clean)
write_wav(out_dir / "audio_rls.wav", rate_noisy, rls_clean)
plot_signals(rate_noisy, noisy, lms_clean, rls_clean)
plot_convergence(rate_noisy, lms_curve, rls_curve)
    \end{lstlisting}
\end{enumerate}

\subsection{Key Parameters}

\begin{table}[h]
\centering
\caption{Parameter Ranges and Recommendations}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Range} & \textbf{Recommended} \\
\midrule
LMS Order & 8-64 & 16-32 \\
LMS $\mu$ & 0.001-0.1 & 0.005-0.01 \\
RLS Order & 4-32 & 8-16 \\
RLS $\lambda$ & 0.9-0.999 & 0.99-0.995 \\
RLS $\delta$ & 0.001-0.1 & 0.01-0.1 \\
\bottomrule
\end{tabular}
\label{tab:parameters}
\end{table}

\end{document}

